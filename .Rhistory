nnet <- train(Y ~ .,
data = newdata[, !(names(newdata) %in% c("ID", "Time"))],
method = "neuralnet",
metric = "RMSE",
trControl = "none",
tuneGrid = tune)
tune <- expand.grid(
layer1 = 3,
layer2 = 3,
layer3 = 3)
nnet <- train(Y ~ .,
data = newdata[, !(names(newdata) %in% c("ID", "Time"))],
method = "neuralnet",
metric = "RMSE",
tuneGrid = tune)
dim(newdata)
tune <- expand.grid(
layer1 = 2,
layer2 = 2,
layer3 = 2)
nnet <- train(Y ~ .,
data = newdata[1:125, !(names(newdata) %in% c("ID", "Time"))],
method = "neuralnet",
metric = "RMSE",
tuneGrid = tune)
warnings()
indices <- CreateSpacetimeFolds(newdata, timevar = "Time", k = 5)
library(CAST)
indices <- CreateSpacetimeFolds(newdata, timevar = "Time", k = 5)
Tt <- length(unique(data[, "Time"]))
Tt <- length(unique(newdata[, "Time"]))
indices <- CreateSpacetimeFolds(data, timevar = "Time", k = Tt)
indices <- CreateSpacetimeFolds(newdata, timevar = "Time", k = Tt)
trainx <- lapply(1:(Tt-2), FUN = function(x) unlist(indices$indexOut[1:x]))
View(indices)
View(indices)
testx <- lapply(1:(Tt-2), FUN = function(x) unlist(indices$indexOut[[x+1]]))
trainx[[2]]]]
trainx[[2]]]
trainx[[2]]
head(data)
head(newdata)
PanelCrossValidation <- function(data, post_period, blocked = pcv_block, metric = "RMSE", trControl = NULL, ML_methods = NULL){
browser()
### Parameter checks
if(!any(class(data) %in% "PanelMLCM")) stop("Invalid class in the PanelCrossValidation function, something is wrong with as.PanelMLCM")
if(!(post_period %in% data[, "Time"])) stop("post_period must be contained in timevar")
if(length(unique(data[, "Time"])) - 2 - blocked < 1) stop("Panel cross validation must be performed in at least one time period")
if(blocked <= 0) stop("The number of 'blocked' time periods for panel cross validation must be at least 1")
if(!metric %in% c("RMSE", "Rsquared")) stop("Metric not allowed, check documentation")
if(is.list(ML_methods)){if(length(ML_methods) != 2) stop("ML_methods must be a list of length 2")}
if(is.list(ML_methods)){if(!names(ML_methods) %in% c("method", "tuneGrid")) stop("ML_methods must be a named list, check documentation")}
### STEP 1. The CAST package is used to generate separate testing sets for each year
Tt <- length(unique(data[, "Time"]))
post_period <- post_period
indices <- CreateSpacetimeFolds(data, timevar = "Time", k = Tt)
trainx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[1:x]))
testx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[[x+1]]))
### STEP 2. Set control function by specifying the training and testing folds that caret will use
###         for cross-validation and tuning of the hyperparameters (i.e., the combination of folds defined above)
if(is.null(trControl)){
ctrl <- trainControl(index = trainx, indexOut = testx)
} else {
ctrl <- trControl # da capire come fare i param checks
}
### STEP 3.  Tune the hyperparameters of each of the ML algorithms via temporal cross-validation
if(is.null(ML_methods)){
# Stochastic gradient boosting
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3),
n.trees=c(500, 1000, 1500, 2000),
shrinkage = seq(0.01, 0.1, by = 0.01),
n.minobsinnode = c(10,20))
set.seed(1)
bo <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "gbm",
metric = metric,
trControl = ctrl,
tuneGrid = gbmGrid,
verbose = FALSE)
# Random forest
set.seed(1)
rf <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "rf",
metric = metric,
search = "grid",
trControl = ctrl,
tuneGrid = expand.grid(mtry = (2:(ncol(data)-3))),
ntree=500)
# LASSO
lasso <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "lasso",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(fraction = seq(0.1, 0.9, by = 0.1)),
preProc=c("center", "scale"))
# PLS
pls <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "pls",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(ncomp = c(1:10)),
preProc=c("center", "scale"))
} else {
lapply(ML_methods, FUN = function(x)(train(Y ~.,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = x$method,
metric = metric,
trControl = ctrl,
tuneGrid = x$tuneGrid)))
}
### STEP 4. Selecting the "best" ML algorithm based on the provided performance metric
m_list <- list(bo = bo, rf = rf, lasso = lasso, pls = pls)
rmse_min <- sapply(m_list, FUN = function(x) min(x$results[, metric]), simplify = T)
ind <- which(rmse_min == min(rmse_min))
### Returning result
return(best = m_list[[ind]])
}
# Organizing the dataset
newdata <- as.PanelMLCM(y = data[, "Y"], timevar = data[, "year"], id = data[, "ID"],
x = data[, !(names(data) %in% c("Y", "ID", "year"))])
# Using the first two years for training and the last two years for testing
indices <- CreateSpacetimeFolds(newdata, timevar = "Time", k = 5)
trainx <- indices$indexOut[1:2]
testx <- indices$indexOut[3:4]
ctrl <- trainControl(index = trainx, indexOut = testx)
# Customized panel cross validation
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, metric = "RMSE", trControl = ctrl)
# Organizing the dataset
newdata <- as.PanelMLCM(y = data[, "Y"], timevar = data[, "year"], id = data[, "ID"],
x = data[, !(names(data) %in% c("Y", "ID", "year"))])
# Using the first two years for training and the last two years for testing
indices <- CreateSpacetimeFolds(newdata, timevar = "Time", k = 5)
trainx <- indices$indexOut[1:2]
testx <- indices$indexOut[3:4]
ctrl <- trainControl(index = trainx, indexOut = testx)
# Customized panel cross validation
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, metric = "RMSE", trControl = ctrl)
# Organizing the dataset
newdata <- as.PanelMLCM(y = data[, "Y"], timevar = data[, "year"], id = data[, "ID"],
x = data[, !(names(data) %in% c("Y", "ID", "year"))])
# Using the first two years for training and the last two years for testing
indices <- CreateSpacetimeFolds(newdata, timevar = "Time", k = 5)
trainx <- indices$indexOut[1:2]
testx <- indices$indexOut[3:4]
ctrl <- trainControl(index = trainx, indexOut = testx)
# Customized panel cross validation
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, metric = "RMSE", trControl = ctrl, blocked = 1)
enet <- list(method = "enet",
tuneGrid = expand.grid(
fraction = seq(0.1, 0.9, by = 0.1),
lambda = seq(0.1, 0.9, by = 0.1)))
linreg <- list(method = "lm",
tuneGrid = expand.grid(
intercept = seq(0, 10, by = 0.5)))
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, metric = "RMSE", ML_methods = list(enet, linreg))
source("~/GitHub/MLCM/R/panelcv.R")
PanelCrossValidation <- function(data, post_period, blocked = 1, metric = "RMSE", trControl = NULL, ML_methods = NULL){
browser()
### Parameter checks
if(!any(class(data) %in% "PanelMLCM")) stop("Invalid class in the PanelCrossValidation function, something is wrong with as.PanelMLCM")
if(!(post_period %in% data[, "Time"])) stop("post_period must be contained in timevar")
if(length(unique(data[, "Time"])) - 2 - blocked < 1) stop("Panel cross validation must be performed in at least one time period")
if(blocked <= 0) stop("The number of 'blocked' time periods for panel cross validation must be at least 1")
if(!metric %in% c("RMSE", "Rsquared")) stop("Metric not allowed, check documentation")
if(is.list(ML_methods)){if(length(ML_methods) != 2) stop("ML_methods must be a list of length 2")}
if(is.list(ML_methods)){if(!names(ML_methods) %in% c("method", "tuneGrid")) stop("ML_methods must be a named list, check documentation")}
### STEP 1. The CAST package is used to generate separate testing sets for each year
Tt <- length(unique(data[, "Time"]))
post_period <- post_period
indices <- CreateSpacetimeFolds(data, timevar = "Time", k = Tt)
trainx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[1:x]))
testx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[[x+1]]))
### STEP 2. Set control function by specifying the training and testing folds that caret will use
###         for cross-validation and tuning of the hyperparameters (i.e., the combination of folds defined above)
if(is.null(trControl)){
ctrl <- trainControl(index = trainx, indexOut = testx)
} else {
ctrl <- trControl # da capire come fare i param checks
}
### STEP 3.  Tune the hyperparameters of each of the ML algorithms via temporal cross-validation
if(is.null(ML_methods)){
# Stochastic gradient boosting
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3),
n.trees=c(500, 1000, 1500, 2000),
shrinkage = seq(0.01, 0.1, by = 0.01),
n.minobsinnode = c(10,20))
set.seed(1)
bo <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "gbm",
metric = metric,
trControl = ctrl,
tuneGrid = gbmGrid,
verbose = FALSE)
# Random forest
set.seed(1)
rf <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "rf",
metric = metric,
search = "grid",
trControl = ctrl,
tuneGrid = expand.grid(mtry = (2:(ncol(data)-3))),
ntree=500)
# LASSO
lasso <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "lasso",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(fraction = seq(0.1, 0.9, by = 0.1)),
preProc=c("center", "scale"))
# PLS
pls <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "pls",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(ncomp = c(1:10)),
preProc=c("center", "scale"))
} else {
lapply(ML_methods, FUN = function(x)(train(Y ~.,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = x$method,
metric = metric,
trControl = ctrl,
tuneGrid = x$tuneGrid)))
}
### STEP 4. Selecting the "best" ML algorithm based on the provided performance metric
m_list <- list(bo = bo, rf = rf, lasso = lasso, pls = pls)
rmse_min <- sapply(m_list, FUN = function(x) min(x$results[, metric]), simplify = T)
ind <- which(rmse_min == min(rmse_min))
### Returning result
return(best = m_list[[ind]])
}
enet <- list(method = "enet",
tuneGrid = expand.grid(
fraction = seq(0.1, 0.9, by = 0.1),
lambda = seq(0.1, 0.9, by = 0.1)))
linreg <- list(method = "lm",
tuneGrid = expand.grid(
intercept = seq(0, 10, by = 0.5)))
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, metric = "RMSE", ML_methods = list(enet, linreg))
ML_methods <- list(enet, linreg)
sapply(ML_methods, FUN = length)
sapply(ML_methods, FUN = length) != 2
all(sapply(ML_methods, FUN = length) != 2)
sapply(ML_methods, FUN = function(x)(names(x)))
sapply(ML_methods, FUN = function(x)(names(x) %in% c("method", "tuneGrid")))
sapply(ML_methods, FUN = function(x)(!names(x) %in% c("method", "tuneGrid")))
sapply(ML_methods, FUN = function(x)(any(!names(x) %in% c("method", "tuneGrid"))))
sapply(ML_methods, FUN = function(x)(any(any(!names(x) %in% c("method", "tuneGrid")))))
any(sapply(ML_methods, FUN = function(x)(any(!names(x) %in% c("method", "tuneGrid")))))
PanelCrossValidation <- function(data, post_period, blocked = 1, metric = "RMSE", trControl = NULL, ML_methods = NULL){
browser()
### Parameter checks
if(!any(class(data) %in% "PanelMLCM")) stop("Invalid class in the PanelCrossValidation function, something is wrong with as.PanelMLCM")
if(!(post_period %in% data[, "Time"])) stop("post_period must be contained in timevar")
if(length(unique(data[, "Time"])) - 2 - blocked < 1) stop("Panel cross validation must be performed in at least one time period")
if(blocked <= 0) stop("The number of 'blocked' time periods for panel cross validation must be at least 1")
if(!metric %in% c("RMSE", "Rsquared")) stop("Metric not allowed, check documentation")
if(is.list(ML_methods)){if(any(sapply(ML_methods, FUN = length) != 2)) stop("'ML_methods' must be a list of methods, each of length 2")}
if(is.list(ML_methods)){
if(any(sapply(ML_methods, FUN = function(x)(any(!names(x) %in% c("method", "tuneGrid")))))) stop("Each method in 'ML_methods' must be a named list, check documentation")}
### STEP 1. The CAST package is used to generate separate testing sets for each year
Tt <- length(unique(data[, "Time"]))
post_period <- post_period
indices <- CreateSpacetimeFolds(data, timevar = "Time", k = Tt)
trainx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[1:x]))
testx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[[x+1]]))
### STEP 2. Set control function by specifying the training and testing folds that caret will use
###         for cross-validation and tuning of the hyperparameters (i.e., the combination of folds defined above)
if(is.null(trControl)){
ctrl <- trainControl(index = trainx, indexOut = testx)
} else {
ctrl <- trControl # da capire come fare i param checks
}
### STEP 3.  Tune the hyperparameters of each of the ML algorithms via temporal cross-validation
if(is.null(ML_methods)){
# Stochastic gradient boosting
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3),
n.trees=c(500, 1000, 1500, 2000),
shrinkage = seq(0.01, 0.1, by = 0.01),
n.minobsinnode = c(10,20))
set.seed(1)
bo <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "gbm",
metric = metric,
trControl = ctrl,
tuneGrid = gbmGrid,
verbose = FALSE)
# Random forest
set.seed(1)
rf <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "rf",
metric = metric,
search = "grid",
trControl = ctrl,
tuneGrid = expand.grid(mtry = (2:(ncol(data)-3))),
ntree=500)
# LASSO
lasso <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "lasso",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(fraction = seq(0.1, 0.9, by = 0.1)),
preProc=c("center", "scale"))
# PLS
pls <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "pls",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(ncomp = c(1:10)),
preProc=c("center", "scale"))
} else {
lapply(ML_methods, FUN = function(x)(train(Y ~.,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = x$method,
metric = metric,
trControl = ctrl,
tuneGrid = x$tuneGrid)))
}
### STEP 4. Selecting the "best" ML algorithm based on the provided performance metric
m_list <- list(bo = bo, rf = rf, lasso = lasso, pls = pls)
rmse_min <- sapply(m_list, FUN = function(x) min(x$results[, metric]), simplify = T)
ind <- which(rmse_min == min(rmse_min))
### Returning result
return(best = m_list[[ind]])
}
enet <- list(method = "enet",
tuneGrid = expand.grid(
fraction = seq(0.1, 0.9, by = 0.1),
lambda = seq(0.1, 0.9, by = 0.1)))
linreg <- list(method = "lm",
tuneGrid = expand.grid(
intercept = seq(0, 10, by = 0.5)))
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, metric = "RMSE", ML_methods = list(enet, linreg))
source("~/GitHub/MLCM/R/panelcv.R")
PanelCrossValidation <- function(data, post_period, blocked = 1, metric = "RMSE", trControl = NULL, ML_methods = NULL){
browser()
### Parameter checks
if(!any(class(data) %in% "PanelMLCM")) stop("Invalid class in the PanelCrossValidation function, something is wrong with as.PanelMLCM")
if(!(post_period %in% data[, "Time"])) stop("post_period must be contained in timevar")
if(length(unique(data[, "Time"])) - 2 - blocked < 1) stop("Panel cross validation must be performed in at least one time period")
if(blocked <= 0) stop("The number of 'blocked' time periods for panel cross validation must be at least 1")
if(!metric %in% c("RMSE", "Rsquared")) stop("Metric not allowed, check documentation")
if(is.list(ML_methods)){if(any(sapply(ML_methods, FUN = length) != 2)) stop("'ML_methods' must be a list of methods, each of length 2")}
if(is.list(ML_methods)){
if(any(sapply(ML_methods, FUN = function(x)(any(!names(x) %in% c("method", "tuneGrid")))))) stop("Each method in 'ML_methods' must be a named list, check documentation")}
### STEP 1. The CAST package is used to generate separate testing sets for each year
Tt <- length(unique(data[, "Time"]))
post_period <- post_period
indices <- CreateSpacetimeFolds(data, timevar = "Time", k = Tt)
trainx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[1:x]))
testx <- lapply(blocked:(Tt-2), FUN = function(x) unlist(indices$indexOut[[x+1]]))
### STEP 2. Set control function by specifying the training and testing folds that caret will use
###         for cross-validation and tuning of the hyperparameters (i.e., the combination of folds defined above)
if(is.null(trControl)){
ctrl <- trainControl(index = trainx, indexOut = testx)
} else {
ctrl <- trControl # da capire come fare i param checks
}
### STEP 3.  Tune the hyperparameters of each of the ML algorithms via temporal cross-validation
if(is.null(ML_methods)){
# STOCHASTIC GRADIENT BOOSTING
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3),
n.trees=c(500, 1000, 1500, 2000),
shrinkage = seq(0.01, 0.1, by = 0.01),
n.minobsinnode = c(10,20))
set.seed(1)
bo <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "gbm",
metric = metric,
trControl = ctrl,
tuneGrid = gbmGrid,
verbose = FALSE)
# RANDOM FOREST
set.seed(1)
rf <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "rf",
metric = metric,
search = "grid",
trControl = ctrl,
tuneGrid = expand.grid(mtry = (2:(ncol(data)-3))),
ntree=500)
# LASSO
lasso <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "lasso",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(fraction = seq(0.1, 0.9, by = 0.1)),
preProc=c("center", "scale"))
# PLS
pls <- train(Y ~ .,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = "pls",
metric = metric,
trControl = ctrl,
tuneGrid = expand.grid(ncomp = c(1:10)),
preProc=c("center", "scale"))
# Storing results in a list
m_list <- list(bo = bo, rf = rf, lasso = lasso, pls = pls)
} else {
m_list <- lapply(ML_methods, FUN = function(x){train(Y ~.,
data = data[, !(names(data) %in% c("ID", "Time"))],
method = x$method,
metric = metric,
trControl = ctrl,
tuneGrid = x$tuneGrid)})
}
### STEP 4. Selecting the "best" ML algorithm based on the provided performance metric
rmse_min <- sapply(m_list, FUN = function(x) min(x$results[, metric]), simplify = T)
ind <- which(rmse_min == min(rmse_min))
### Returning result
return(best = m_list[[ind]])
}
enet <- list(method = "enet",
tuneGrid = expand.grid(
fraction = seq(0.1, 0.9, by = 0.1),
lambda = seq(0.1, 0.9, by = 0.1)))
linreg <- list(method = "lm",
tuneGrid = expand.grid(
intercept = seq(0, 10, by = 0.5)))
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, metric = "RMSE", ML_methods = list(enet, linreg))
source("~/GitHub/MLCM/R/panelcv.R")
source("~/GitHub/MLCM/R/panelcv.R")
source("~/GitHub/MLCM/R/panelcv.R")
source("~/GitHub/MLCM/R/panelcv.R")
class(ctrl)
source("~/GitHub/MLCM/R/panelcv.R")
source("~/GitHub/MLCM/R/panelcv.R")
library(MLCM)
install.packages("Rtools")
library(MLCM)
class(pcv)
source("~/GitHub/MLCM/R/panelcv.R")
library(MLCM)
newdata <- as.PanelMLCM(y = data[, "Y"], timevar = data[, "year"], id = data[, "ID"],
x = data[, !(names(data) %in% c("Y", "ID", "year"))])
# Using the first two years for training and the last two years for testing
indices <- CreateSpacetimeFolds(newdata, timevar = "Time", k = 5)
trainx <- indices$indexOut[1:2]
testx <- indices$indexOut[3:4]
ctrl <- trainControl(index = trainx, indexOut = testx)
enet <- list(method = "enet",
tuneGrid = expand.grid(
fraction = seq(0.1, 0.9, by = 0.1),
lambda = seq(0.1, 0.9, by = 0.1)))
linreg <- list(method = "lm",
tuneGrid = expand.grid(
intercept = seq(0, 10, by = 0.5)))
pcv <- PanelCrossValidation(data = newdata, post_period = 2020, ML_methods = list(enet, linreg))
causal <- MLCM(data = data, post_period = 2020, PCV = pcv)
library(MLCM)
causal <- MLCM(data = data, post_period = 2020, PCV = pcv)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
!is.null(y)
class(y)
library(MLCM)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
library(MLCM)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
!(is.null(y) | class(y) == "character")
library(MLCM)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
y %in% colnames(data)
library(MLCM)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
library(MLCM)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
post_period %in% data[, timevar]
post_period %in% data[, "Time"]
post_period %in% data[, timevar] | post_period %in% data[, "Time"]
2020 %in% data[, timevar] | 2020 %in% data[, "Time"]
library(MLCM)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv)
causal <- MLCM(data = newdata, post_period = 2020, PCV = pcv, inf_type = "block")
causal$ate
causal$ate.lower
causal$ate.upper
library(MLCM)
source("~/GitHub/MLCM/R/panelcv.R")
library(MLCM)
library(MLCM)
fit <- MLCM(data = data, y = "Y", timevar = "year", id = "ID", post_period = 2020, inf_type = "classic", nboot = 10)
library(MLCM)
fit <- MLCM(data = data, y = "Y", timevar = "year", id = "ID", post_period = 2020, inf_type = "classic", nboot = 10)
library(MLCM)
fit <- MLCM(data = data, y = "Y", timevar = "year", id = "ID", post_period = 2020, inf_type = "classic", nboot = 10)
fit <- MLCM(data = data, y = "Y", timevar = "year", id = "ID", post_period = 2020, inf_type = "classic", nboot = 10)
timevar
!post_period %in% data[, timevar]
!post_period %in% data[, timevar]
if(!post_period %in% data[, timevar]){print("error")}
library(MLCM)
fit <- MLCM(data = data, y = "Y", timevar = "year", id = "ID", post_period = 2020, inf_type = "classic", nboot = 10)
library(MLCM)
